0it [00:00, ?it/s]  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 8192/170498071 [00:00<37:19, 76119.68it/s]  0%|          | 40960/170498071 [00:00<28:58, 98069.77it/s]  0%|          | 90112/170498071 [00:00<22:09, 128144.07it/s]  0%|          | 221184/170498071 [00:00<16:12, 175048.95it/s]  0%|          | 450560/170498071 [00:00<11:44, 241464.75it/s]  1%|          | 909312/170498071 [00:01<08:23, 336672.05it/s]  1%|          | 1826816/170498071 [00:01<05:56, 472857.60it/s]  2%|▏         | 3678208/170498071 [00:01<04:09, 667537.39it/s]  4%|▍         | 6529024/170498071 [00:01<02:53, 943107.47it/s]  6%|▌         | 9658368/170498071 [00:01<02:01, 1328601.53it/s]  7%|▋         | 12607488/170498071 [00:01<01:24, 1858823.34it/s]  9%|▉         | 15409152/170498071 [00:01<01:00, 2575492.58it/s] 11%|█         | 18472960/170498071 [00:01<00:42, 3540167.36it/s] 13%|█▎        | 21553152/170498071 [00:01<00:31, 4799499.91it/s] 14%|█▍        | 24633344/170498071 [00:02<00:22, 6390696.29it/s] 16%|█▌        | 27566080/170498071 [00:02<00:17, 8284426.33it/s] 18%|█▊        | 30531584/170498071 [00:02<00:13, 10467729.16it/s] 20%|█▉        | 33513472/170498071 [00:02<00:10, 12849109.90it/s] 21%|██▏       | 36511744/170498071 [00:02<00:08, 15288488.95it/s] 23%|██▎       | 39559168/170498071 [00:02<00:07, 17699203.45it/s] 25%|██▍       | 42622976/170498071 [00:02<00:06, 19904838.88it/s] 27%|██▋       | 45703168/170498071 [00:02<00:05, 21831920.00it/s] 28%|██▊       | 48480256/170498071 [00:02<00:05, 22752395.98it/s] 30%|███       | 51453952/170498071 [00:03<00:04, 23830187.37it/s] 32%|███▏      | 54501376/170498071 [00:03<00:04, 24758717.81it/s] 34%|███▍      | 57548800/170498071 [00:03<00:04, 25103951.55it/s] 36%|███▌      | 60612608/170498071 [00:03<00:04, 26494704.47it/s] 37%|███▋      | 63660032/170498071 [00:03<00:03, 26911885.89it/s] 39%|███▉      | 66740224/170498071 [00:03<00:03, 27299212.20it/s] 41%|████      | 69869568/170498071 [00:03<00:03, 27695735.24it/s] 43%|████▎     | 72695808/170498071 [00:03<00:03, 26443234.29it/s] 44%|████▍     | 75702272/170498071 [00:03<00:03, 26790090.41it/s] 46%|████▌     | 78782464/170498071 [00:04<00:03, 27201416.49it/s] 48%|████▊     | 81895424/170498071 [00:04<00:03, 27596944.93it/s] 50%|████▉     | 84992000/170498071 [00:04<00:03, 27822235.51it/s] 52%|█████▏    | 88104960/170498071 [00:04<00:02, 28039032.62it/s] 53%|█████▎    | 91119616/170498071 [00:04<00:02, 27917579.24it/s] 55%|█████▌    | 93986816/170498071 [00:04<00:02, 27327007.35it/s] 57%|█████▋    | 97050624/170498071 [00:04<00:02, 27550444.59it/s] 59%|█████▊    | 100114432/170498071 [00:04<00:02, 27705703.68it/s] 61%|██████    | 103211008/170498071 [00:04<00:02, 27897942.99it/s] 62%|██████▏   | 106323968/170498071 [00:05<00:02, 28092106.68it/s] 64%|██████▍   | 109436928/170498071 [00:05<00:02, 28220704.57it/s] 66%|██████▌   | 112500736/170498071 [00:05<00:02, 28174293.65it/s] 68%|██████▊   | 115613696/170498071 [00:05<00:01, 28279723.62it/s] 70%|██████▉   | 118661120/170498071 [00:05<00:01, 28181917.92it/s] 71%|███████▏  | 121544704/170498071 [00:05<00:01, 27342694.94it/s] 73%|███████▎  | 124624896/170498071 [00:05<00:01, 27596623.59it/s] 75%|███████▍  | 127754240/170498071 [00:05<00:01, 27903697.96it/s] 77%|███████▋  | 130834432/170498071 [00:05<00:01, 28000119.98it/s] 79%|███████▊  | 133898240/170498071 [00:06<00:01, 28025473.52it/s] 80%|████████  | 137027584/170498071 [00:06<00:01, 28225911.86it/s] 82%|████████▏ | 140091392/170498071 [00:06<00:01, 28170736.41it/s] 84%|████████▍ | 143171584/170498071 [00:06<00:00, 28195779.32it/s] 86%|████████▌ | 146317312/170498071 [00:06<00:00, 28067756.62it/s] 88%|████████▊ | 149397504/170498071 [00:06<00:00, 28427992.03it/s] 89%|████████▉ | 152461312/170498071 [00:06<00:00, 28326445.33it/s] 91%|█████████ | 155377664/170498071 [00:06<00:00, 26774740.82it/s] 93%|█████████▎| 158457856/170498071 [00:06<00:00, 27197292.17it/s] 95%|█████████▍| 161521664/170498071 [00:07<00:00, 27454187.45it/s] 97%|█████████▋| 164634624/170498071 [00:07<00:00, 27765765.66it/s] 98%|█████████▊| 167714816/170498071 [00:07<00:00, 27914186.41it/s]170500096it [00:10, 16292657.54it/s]                               
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())
/raid/condor/lib/condor/execute/dir_1587817/torchlars/lars.py:141: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554793803/work/torch/csrc/utils/python_arg_parser.cpp:1005.)
  p.grad.add_(weight_decay, p.data)
